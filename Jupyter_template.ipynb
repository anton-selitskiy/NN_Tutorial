{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Jupiter_template.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHMAh51wYwNo"
      },
      "source": [
        "## For using Google Drive use next two cells (you don't need them in .py file)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp2lQRLdYuCY",
        "outputId": "d837a8ee-b98a-45df-be8b-fa59c4ff583b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVf1VxyxZiST"
      },
      "source": [
        "## Path to your data on Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQHlBpJ4YxYU",
        "outputId": "9245e663-6af0-4841-dd78-dee09ea531bd"
      },
      "source": [
        "%cd /content/gdrive/My Drive/HW5"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/HW5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpeP3-2HbzkR"
      },
      "source": [
        "from __future__ import print_function, division, absolute_import, unicode_literals\n",
        "import six\n",
        "import os\n",
        "import numpy as np\n",
        "import Data\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "#import argparse       #you don't need it in Colab\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm, trange"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thhtwm28bzkS"
      },
      "source": [
        "class ModelSingleStep(torch.nn.Module):\n",
        "    def __init__(self, blockSize):\n",
        "        super(ModelSingleStep, self).__init__()\n",
        "        self.blockSize = blockSize\n",
        "\n",
        "        ###################################\n",
        "        # define your layers here\n",
        "        ###################################\n",
        "\n",
        "        ###################################\n",
        "\n",
        "        self.initParams()\n",
        "\n",
        "    def initParams(self):\n",
        "        for param in self.parameters():\n",
        "            if len(param.shape) > 1:\n",
        "                torch.nn.init.xavier_normal_(param)\n",
        "\n",
        "    def encode(self, x):\n",
        "        ###################################\n",
        "        # implement the encoder\n",
        "        ###################################\n",
        "\n",
        "        ###################################\n",
        "        return h\n",
        "\n",
        "    def decode(self, h):\n",
        "        ###################################\n",
        "        # implement the decoder\n",
        "        ###################################\n",
        "\n",
        "        ###################################\n",
        "        return o\n",
        "\n",
        "    def forward(self, x):\n",
        "        # glue the encoder and the decoder together\n",
        "        h = self.encode(x)\n",
        "        o = self.decode(h)\n",
        "        return o\n",
        "\n",
        "    def process(self, magnitude):\n",
        "        # process the whole chunk of spectrogram at run time\n",
        "        result = magnitude.copy()\n",
        "        with torch.no_grad():\n",
        "            nFrame = magnitude.shape[1]\n",
        "            for i in range(nFrame):\n",
        "                result[:, i] = magnitude[:, i] * self.forward(torch.from_numpy(magnitude[:, i].reshape(1, -1))).numpy()\n",
        "        return result"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KIxXVgVbzkT"
      },
      "source": [
        "def validate(model, dataloader):\n",
        "    validationLoss = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Each time fetch a batch of samples from the dataloader\n",
        "        for sample in dataloader:\n",
        "            pass\n",
        "    ######################################################################################\n",
        "    # Implement here your validation loop. It should be similar to your train loop\n",
        "    # without the backpropagation steps\n",
        "    ######################################################################################\n",
        "\n",
        "    model.train()\n",
        "    return validationLoss"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j-KcwI5bzkU"
      },
      "source": [
        "def saveFigure(result, target, mixture):\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.pcolormesh(np.log(1e-4 + result), vmin=-300 / 20, vmax=10 / 20)\n",
        "    plt.title('estimated')\n",
        "\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.pcolormesh(np.log(1e-4 + target.cpu()[0, :, :].numpy()), vmin=-300 / 20, vmax=10 / 20)\n",
        "    plt.title('vocal')\n",
        "    plt.subplot(3, 1, 3)\n",
        "\n",
        "    plt.pcolormesh(np.log(1e-4 + mixture.cpu()[0, :, :].numpy()), vmin=-300 / 20, vmax=10 / 20)\n",
        "    plt.title('mixture')\n",
        "\n",
        "    plt.savefig(\"result_feedforward.png\")\n",
        "    plt.gcf().clear()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtpQHS-va-IJ"
      },
      "source": [
        "## This is \"\\__main__\" function. Next two cells we need only in Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nErNUWM0bzkV"
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.blockSize = 4096\n",
        "        self.hopSize = 2048\n",
        "    # how many audio files to process fetched at each time, modify it if OOM error\n",
        "        self.batchSize = 8\n",
        "    # set the learning rate, default value is 0.0001\n",
        "        self.lr = 1e-4\n",
        "    # Path to the dataset, modify it accordingly\n",
        "        self.dataset = './DSD100'   # \"/content/gdrive/My Drive/HW5/DSD100\"  \n",
        "    # set --load to 1, if you want to restore weights from a previous trained model\n",
        "        self.load = 0\n",
        "    # path of the checkpoint that you want to restore\n",
        "        self.checkpoint = 'savedModel_feedForward_best.pt'\n",
        "        self.seed = 555"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jCOlCL9bzkV"
      },
      "source": [
        "args = Arguments()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOrYi53TbzkW"
      },
      "source": [
        "# Random seeds, for reproducibility\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "fs = 32000\n",
        "blockSize = args.blockSize\n",
        "hopSize = args.hopSize\n",
        "PATH_DATASET = args.dataset\n",
        "batchSize = args.batchSize\n",
        "minValLoss = np.inf"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBuoM54qbzkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e868b0d-c567-4a99-f511-640bc5dbf585"
      },
      "source": [
        "# transformation pipeline for training data\n",
        "transformTrain = transforms.Compose([\n",
        "    # Randomly rescale the training data\n",
        "    Data.Transforms.Rescale(0.8, 1.2),\n",
        "\n",
        "    # Randomly shift the beginning of the training data, because we always do chunking for training in this case\n",
        "    Data.Transforms.RandomShift(fs * 30),\n",
        "\n",
        "    # transform the raw audio into spectrogram\n",
        "    Data.Transforms.MakeMagnitudeSpectrum(blockSize=blockSize, hopSize=hopSize),\n",
        "\n",
        "    # shuffle all frames of a song for training the single-frame model , \n",
        "    #NB!!!! remove this line for training a temporal sequence model\n",
        "    Data.Transforms.ShuffleFrameOrder()\n",
        "])\n",
        "\n",
        "# transformation pipeline for training data. Here, we don't have to use any augmentation/regularization techqniques\n",
        "transformVal = transforms.Compose([\n",
        "    # transform the raw audio into spectrogram\n",
        "    Data.Transforms.MakeMagnitudeSpectrum(blockSize=blockSize, hopSize=hopSize),\n",
        "])\n",
        "\n",
        "# initialize dataloaders for training and validation data, every sample loaded will go thourgh the preprocessing pipeline defined by the above transformations\n",
        "# workers will restart after each epoch, which takes a lot of time. repetition = 8  repeats the dataset 8 times in order to reduce the waiting time\n",
        "# so, in this case,  1 epoch is equal to 8 epochs. For validation data, there is not point in repeating the dataset.\n",
        "datasetTrain = Data.DSD100Dataset(PATH_DATASET, split='Train', mono=True, transform=transformTrain, repetition=8)\n",
        "datasetValid = Data.DSD100Dataset(PATH_DATASET, split='Valid', mono=True, transform=transformVal, repetition=1)\n",
        "\n",
        "# initialize the data loader\n",
        "# num_workers means how many workers are used to prefetch the data, reduce num_workers if OOM error\n",
        "dataloaderTrain = torch.utils.data.DataLoader(datasetTrain, batch_size=batchSize, shuffle=True, num_workers=4,\n",
        "                                              collate_fn=Data.collate_fn)\n",
        "dataloaderValid = torch.utils.data.DataLoader(datasetValid, batch_size=10, shuffle=False, num_workers=0,\n",
        "                                              collate_fn=Data.collate_fn)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fty_p4wvbzkX"
      },
      "source": [
        "# initialize the Model\n",
        "model = ModelSingleStep(blockSize)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d6uNIpFbzkY"
      },
      "source": [
        "# if you want to restore your previous saved model, set --load argument to 1\n",
        "if args.load == 1:\n",
        "    checkpoint = torch.load(args.checkpoint)\n",
        "    minValLoss = checkpoint['minValLoss']\n",
        "    model.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ax8Rwv2bzkY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f7316fe-d842-42a0-8ed1-da755196545e"
      },
      "source": [
        "# determine if cuda is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModelSingleStep(\n",
              "  (encoder): Sequential(\n",
              "    (0): Linear(in_features=2049, out_features=1000, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.01)\n",
              "    (2): Linear(in_features=1000, out_features=400, bias=True)\n",
              "    (3): LeakyReLU(negative_slope=0.01)\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): Linear(in_features=400, out_features=1000, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.01)\n",
              "    (2): Linear(in_features=1000, out_features=2049, bias=True)\n",
              "    (3): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYQW4YxfjSN6"
      },
      "source": [
        "## Next cell will not work unless you define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7A4I2jGbzka"
      },
      "source": [
        "# initialize the optimizer for paramters\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnVA02e2bzka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ae771a-760c-487a-dcfa-22c7d32048bc"
      },
      "source": [
        "model.train(mode=True)\n",
        "\n",
        "lossMovingAveraged = -1\n",
        "\n",
        "####################################\n",
        "# The main loop of training\n",
        "####################################\n",
        "for epoc in range(2):\n",
        "    iterator = iter(dataloaderTrain)\n",
        "    with trange(len(dataloaderTrain)) as t:\n",
        "        for idx in t:\n",
        "            # Each time fetch a batch of samples from the dataloader\n",
        "            sample = next(iterator)\n",
        "            # the progress of training in the current epoch\n",
        "\n",
        "            # Remember to clear the accumulated gradient each time you perfrom optimizer.step()\n",
        "            model.zero_grad()\n",
        "\n",
        "            # read the input and the fitting target into the device\n",
        "            mixture = sample['mixture'].to(device)\n",
        "            target = sample['vocal'].to(device)\n",
        "\n",
        "            seqLen = mixture.shape[2]\n",
        "            winLen = mixture.shape[1]\n",
        "            currentBatchSize = mixture.shape[0]\n",
        "\n",
        "            # store the result for the first one for debugging purpose\n",
        "            result = torch.zeros((winLen, seqLen), dtype=torch.float32)\n",
        "\n",
        "            #################################\n",
        "            # Fill the rest of the code here#\n",
        "            #################################\n",
        "\n",
        "            # store your smoothed loss here\n",
        "            lossMovingAveraged = 0\n",
        "            # this is used to set a description in the tqdm progress bar\n",
        "            t.set_description(f\"epoc : {epoc}, loss {lossMovingAveraged}\")\n",
        "            # save the model\n",
        "\n",
        "        # plot the first one in the batch for debuging purpose\n",
        "        saveFigure(result, target, mixture)\n",
        "        # Most likely, your 'result' will be a batch, so, you can use here \n",
        "        # the first element of the batch instead of 'result':\n",
        "        # result.cpu().detach().numpy()[0, :, :]\n",
        "\n",
        "    # create a checkpoint of the current state of training\n",
        "    checkpoint = {\n",
        "        'state_dict': model.state_dict(),\n",
        "        'minValLoss': minValLoss,\n",
        "    }\n",
        "    # save the last checkpoint\n",
        "    torch.save(checkpoint, 'savedModel_feedForward_last.pt')\n",
        "\n",
        "    #### Calculate validation loss\n",
        "    valLoss = validate(model, dataloaderValid)\n",
        "    print(f\"validation Loss = {valLoss:.4f}\")\n",
        "\n",
        "    if valLoss < minValLoss:\n",
        "        minValLoss = valLoss\n",
        "        # then save checkpoint\n",
        "        checkpoint = {\n",
        "            'state_dict': model.state_dict(),\n",
        "            'minValLoss': minValLoss,\n",
        "        }\n",
        "        torch.save(checkpoint, 'savedModel_feedForward_best.pt')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "  0%|          | 0/30 [00:04<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdg3g2QJdMf7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}