{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import, unicode_literals\n",
    "import six\n",
    "import os\n",
    "import numpy as np\n",
    "import Data\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSingleStep(torch.nn.Module):\n",
    "    def __init__(self, blockSize):\n",
    "        super(ModelSingleStep, self).__init__()\n",
    "        self.blockSize = blockSize\n",
    "\n",
    "        ###################################\n",
    "        # define your layers here\n",
    "        ###################################\n",
    "\n",
    "        ###################################\n",
    "\n",
    "        self.initParams()\n",
    "\n",
    "    def initParams(self):\n",
    "        for param in self.parameters():\n",
    "            if len(param.shape) > 1:\n",
    "                torch.nn.init.xavier_normal_(param)\n",
    "\n",
    "    def encode(self, x):\n",
    "        ###################################\n",
    "        # implement the encoder\n",
    "        ###################################\n",
    "\n",
    "        ###################################\n",
    "        return h\n",
    "\n",
    "    def decode(self, h):\n",
    "        ###################################\n",
    "        # implement the decoder\n",
    "        ###################################\n",
    "\n",
    "        ###################################\n",
    "        return o\n",
    "\n",
    "    def forward(self, x):\n",
    "        # glue the encoder and the decoder together\n",
    "        h = self.encode(x)\n",
    "        o = self.decode(h)\n",
    "        return o\n",
    "\n",
    "    def process(self, magnitude):\n",
    "        # process the whole chunk of spectrogram at run time\n",
    "        result = magnitude.copy()\n",
    "        with torch.no_grad():\n",
    "            nFrame = magnitude.shape[1]\n",
    "            for i in range(nFrame):\n",
    "                result[:, i] = magnitude[:, i] * self.forward(torch.from_numpy(magnitude[:, i].reshape(1, -1))).numpy()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Each time fetch a batch of samples from the dataloader\n",
    "        for sample in dataloader:\n",
    "            pass\n",
    "    ######################################################################################\n",
    "    # Implement here your validation loop. It should be similar to your train loop\n",
    "    # without the backpropagation steps\n",
    "    ######################################################################################\n",
    "\n",
    "    model.train()\n",
    "    return validationLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveFigure(result, target, mixture):\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.pcolormesh(np.log(1e-4 + result), vmin=-300 / 20, vmax=10 / 20)\n",
    "    plt.title('estimated')\n",
    "\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.pcolormesh(np.log(1e-4 + target.cpu()[0, :, :].numpy()), vmin=-300 / 20, vmax=10 / 20)\n",
    "    plt.title('vocal')\n",
    "    plt.subplot(3, 1, 3)\n",
    "\n",
    "    plt.pcolormesh(np.log(1e-4 + mixture.cpu()[0, :, :].numpy()), vmin=-300 / 20, vmax=10 / 20)\n",
    "    plt.title('mixture')\n",
    "\n",
    "    plt.savefig(\"result_feedforward.png\")\n",
    "    plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.blockSize = 4096\n",
    "        self.hopSize = 2048\n",
    "    # how many audio files to process fetched at each time, modify it if OOM error\n",
    "        self.batchSize = 8\n",
    "    # set the learning rate, default value is 0.0001\n",
    "        self.lr = 1e-4\n",
    "    # Path to the dataset, modify it accordingly\n",
    "        self.dataset = './DSD100' #'/home/aselitsk/PycharmProjects/pythonProject/DSD100'\n",
    "    # set --load to 1, if you want to restore weights from a previous trained model\n",
    "        self.load = 0\n",
    "    # path of the checkpoint that you want to restore\n",
    "        self.checkpoint = 'savedModel_feedForward_best.pt'\n",
    "        self.seed = 555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seeds, for reproducibility\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fs = 32000\n",
    "blockSize = args.blockSize\n",
    "hopSize = args.hopSize\n",
    "PATH_DATASET = args.dataset\n",
    "batchSize = args.batchSize\n",
    "minValLoss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation pipeline for training data\n",
    "transformTrain = transforms.Compose([\n",
    "    # Randomly rescale the training data\n",
    "    Data.Transforms.Rescale(0.8, 1.2),\n",
    "\n",
    "    # Randomly shift the beginning of the training data, because we always do chunking for training in this case\n",
    "    Data.Transforms.RandomShift(fs * 30),\n",
    "\n",
    "    # transform the raw audio into spectrogram\n",
    "    Data.Transforms.MakeMagnitudeSpectrum(blockSize=blockSize, hopSize=hopSize),\n",
    "\n",
    "    # shuffle all frames of a song for training the single-frame model , remove this line for training a temporal sequence model\n",
    "    Data.Transforms.ShuffleFrameOrder()\n",
    "])\n",
    "\n",
    "# transformation pipeline for training data. Here, we don't have to use any augmentation/regularization techqniques\n",
    "transformVal = transforms.Compose([\n",
    "    # transform the raw audio into spectrogram\n",
    "    Data.Transforms.MakeMagnitudeSpectrum(blockSize=blockSize, hopSize=hopSize),\n",
    "])\n",
    "\n",
    "# initialize dataloaders for training and validation data, every sample loaded will go thourgh the preprocessing pipeline defined by the above transformations\n",
    "# workers will restart after each epoch, which takes a lot of time. repetition = 8  repeats the dataset 8 times in order to reduce the waiting time\n",
    "# so, in this case,  1 epoch is equal to 8 epochs. For validation data, there is not point in repeating the dataset.\n",
    "datasetTrain = Data.DSD100Dataset(PATH_DATASET, split='Train', mono=True, transform=transformTrain, repetition=8)\n",
    "datasetValid = Data.DSD100Dataset(PATH_DATASET, split='Valid', mono=True, transform=transformVal, repetition=1)\n",
    "\n",
    "# initialize the data loader\n",
    "# num_workers means how many workers are used to prefetch the data, reduce num_workers if OOM error\n",
    "dataloaderTrain = torch.utils.data.DataLoader(datasetTrain, batch_size=batchSize, shuffle=True, num_workers=4,\n",
    "                                              collate_fn=Data.collate_fn)\n",
    "dataloaderValid = torch.utils.data.DataLoader(datasetValid, batch_size=10, shuffle=False, num_workers=0,\n",
    "                                              collate_fn=Data.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the Model\n",
    "model = ModelSingleStep(blockSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to restore your previous saved model, set --load argument to 1\n",
    "if args.load == 1:\n",
    "    checkpoint = torch.load(args.checkpoint)\n",
    "    minValLoss = checkpoint['minValLoss']\n",
    "    model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelSingleStep()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine if cuda is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/local_scratch/7646351/ipykernel_2771/3068664427.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# initialize the optimizer for paramters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/software/miniconda3/4.9.2/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[1;32m     46\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[1;32m     47\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad)\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/miniconda3/4.9.2/envs/pytorch/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "# initialize the optimizer for paramters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(mode=True)\n",
    "\n",
    "lossMovingAveraged = -1\n",
    "\n",
    "####################################\n",
    "# The main loop of training\n",
    "####################################\n",
    "for epoc in range(100):\n",
    "    iterator = iter(dataloaderTrain)\n",
    "    with trange(len(dataloaderTrain)) as t:\n",
    "        for idx in t:\n",
    "            # Each time fetch a batch of samples from the dataloader\n",
    "            sample = next(iterator)\n",
    "            # the progress of training in the current epoch\n",
    "\n",
    "            # Remember to clear the accumulated gradient each time you perfrom optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "            # read the input and the fitting target into the device\n",
    "            mixture = sample['mixture'].to(device)\n",
    "            target = sample['vocal'].to(device)\n",
    "\n",
    "            seqLen = mixture.shape[2]\n",
    "            winLen = mixture.shape[1]\n",
    "            currentBatchSize = mixture.shape[0]\n",
    "\n",
    "            # store the result for the first one for debugging purpose\n",
    "            result = torch.zeros((winLen, seqLen), dtype=torch.float32)\n",
    "\n",
    "            #################################\n",
    "            # Fill the rest of the code here#\n",
    "            #################################\n",
    "\n",
    "            # store your smoothed loss here\n",
    "            lossMovingAveraged = 0\n",
    "            # this is used to set a description in the tqdm progress bar\n",
    "            t.set_description(f\"epoc : {epoc}, loss {lossMovingAveraged}\")\n",
    "            # save the model\n",
    "\n",
    "        # plot the first one in the batch for debuging purpose\n",
    "        saveFigure(result, target, mixture)\n",
    "\n",
    "    # create a checkpoint of the current state of training\n",
    "    checkpoint = {\n",
    "        'state_dict': model.state_dict(),\n",
    "        'minValLoss': minValLoss,\n",
    "    }\n",
    "    # save the last checkpoint\n",
    "    torch.save(checkpoint, 'savedModel_feedForward_last.pt')\n",
    "\n",
    "    #### Calculate validation loss\n",
    "    valLoss = validate(model, dataloaderValid)\n",
    "    print(f\"validation Loss = {valLoss:.4f}\")\n",
    "\n",
    "    if valLoss < minValLoss:\n",
    "        minValLoss = valLoss\n",
    "        # then save checkpoint\n",
    "        checkpoint = {\n",
    "            'state_dict': model.state_dict(),\n",
    "            'minValLoss': minValLoss,\n",
    "        }\n",
    "        torch.save(checkpoint, 'savedModel_feedForward_best.pt')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.9.1)",
   "language": "python",
   "name": "pytorch-1.9.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
